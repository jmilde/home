<!DOCTYPE html PUBLIC ""
    "">
<html><head><meta charset="UTF-8" /><title>ysmiraak.tensor documentation</title><link rel="stylesheet" type="text/css" href="css/default.css" /><link rel="stylesheet" type="text/css" href="css/highlight.css" /><script type="text/javascript" src="js/highlight.min.js"></script><script type="text/javascript" src="js/jquery.min.js"></script><script type="text/javascript" src="js/page_effects.js"></script><script>hljs.initHighlightingOnLoad();</script></head><body><div id="header"><h2>Generated by <a href="https://github.com/weavejester/codox">Codox</a></h2><h1><a href="index.html"><span class="project-title"><span class="project-name">Ysmiraak</span> <span class="project-version">2020.05.17</span></span></a></h1></div><div class="sidebar primary"><h3 class="no-link"><span class="inner">Project</span></h3><ul class="index-link"><li class="depth-1 "><a href="index.html"><div class="inner">Index</div></a></li></ul><h3 class="no-link"><span class="inner">Namespaces</span></h3><ul><li class="depth-1"><a href="ysmiraak.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>ysmiraak</span></div></a></li><li class="depth-2 branch"><a href="ysmiraak.core.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>core</span></div></a></li><li class="depth-2 branch"><a href="ysmiraak.lang.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>lang</span></div></a></li><li class="depth-2 branch"><a href="ysmiraak.native.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>native</span></div></a></li><li class="depth-2 branch"><a href="ysmiraak.repl.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>repl</span></div></a></li><li class="depth-2 branch"><a href="ysmiraak.specter.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>specter</span></div></a></li><li class="depth-2 branch"><a href="ysmiraak.sugar.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>sugar</span></div></a></li><li class="depth-2 current"><a href="ysmiraak.tensor.html"><div class="inner"><span class="tree"><span class="top"></span><span class="bottom"></span></span><span>tensor</span></div></a></li></ul></div><div class="sidebar secondary"><h3><a href="#top"><span class="inner">Public Vars</span></a></h3><ul><li class="depth-1"><a href="ysmiraak.tensor.html#var-dims"><div class="inner"><span>dims</span></div></a></li><li class="depth-1"><a href="ysmiraak.tensor.html#var-fibx"><div class="inner"><span>fibx</span></div></a></li><li class="depth-1"><a href="ysmiraak.tensor.html#var-flix"><div class="inner"><span>flix</span></div></a></li><li class="depth-1"><a href="ysmiraak.tensor.html#var-inat"><div class="inner"><span>inat</span></div></a></li><li class="depth-1"><a href="ysmiraak.tensor.html#var-jnat"><div class="inner"><span>jnat</span></div></a></li><li class="depth-1"><a href="ysmiraak.tensor.html#var-mapx"><div class="inner"><span>mapx</span></div></a></li><li class="depth-1"><a href="ysmiraak.tensor.html#var-plux"><div class="inner"><span>plux</span></div></a></li><li class="depth-1"><a href="ysmiraak.tensor.html#var-rank"><div class="inner"><span>rank</span></div></a></li><li class="depth-1"><a href="ysmiraak.tensor.html#var-size"><div class="inner"><span>size</span></div></a></li><li class="depth-1"><a href="ysmiraak.tensor.html#var-subx"><div class="inner"><span>subx</span></div></a></li><li class="depth-1"><a href="ysmiraak.tensor.html#var-tenx"><div class="inner"><span>tenx</span></div></a></li></ul></div><div class="namespace-docs" id="content"><h1 class="anchor" id="top">ysmiraak.tensor</h1><div class="doc"><div class="markdown"><p>playing with your tensors may cause blindness.</p></div></div><div class="public anchor" id="var-dims"><h3>dims</h3><div class="usage"><code>(dims x)</code><code>(dims ds x)</code></div><div class="doc"><div class="markdown"><p>returns the shape <code>(: Vect r Nat)</code> of tensor <em>x</em>, or reshapes it to new shape <em>ds</em>, if its size can be preserved.</p>
<p>unlike common reshaping rules which conveniently allow one -1 in the shape to specify an unknown dimension to be dynamically filled, the new shape here may contain any number of negative integers. any unassigned size is distributed among the negative axis according to their relative magnitude.</p>
<p>for example with <code>(dims ? (vec (range 6)))</code> the shape <code>[-2 3 -1]</code> produces a tensor of shape <code>[2 3 1]</code>, whereas <code>[-1 3 -1]</code> gets an exception message.</p>
<p>the purpose of this design is to simplify the logic of the implementation and to avoid special handling for edge cases, for example what happens when -1 meets a tensor of size zero, and what happens when multiple -1s are given but the remaining positive dimensions fill the size perfectly.</p>
<p>for other common ndarray manipulations, see <a href="ysmiraak.tensor.html#var-rank">rank</a> for flattening, <a href="ysmiraak.tensor.html#var-size">size</a> for broadcasting, <a href="ysmiraak.tensor.html#var-flix">flix</a> for transposition, and <a href="ysmiraak.tensor.html#var-subx">subx</a> for slicing and indexing.</p></div></div><div class="src-link"><a href="https://github.com/ysmiraak/home/blob/master/src/ysmiraak/tensor.clj#L81">view source</a></div></div><div class="public anchor" id="var-fibx"><h3>fibx</h3><div class="usage"><code>(fibx x)</code></div><div class="doc"><div class="markdown"><p>tensor fiber.</p>
<p>consider tensor type <code>Tsor (k : Type) (r : Nat) (d : Vect Nat r)</code> equivalently expressed as <code>fmap FinSet d -&gt; k</code>; its fiber (inverse image) function type is <code>k -&gt; PowSet (fmap FinSet d -&gt; k)</code>; or</p>
<pre><code>Tsor k r = Vect Nat r -&gt; k
k -&gt; Vect (Vect Nat r) ?
</code></pre>
<p>with some simplification.</p>
<p>now consider only truthy and falsey values of <code>k</code>, this function returns the truthy fiber, aka indices of truthy values in tensor <em>x</em> where each index is a vector of <em>r</em> natural numbers corresponding to the axes. note that <em>0</em> and <em>0.0</em> are considered truthy.</p>
<p>it is then possible to retrieve the fiber of arbitrary values by converting the values to booleans with <a href="ysmiraak.tensor.html#var-plux">plux</a> or <a href="ysmiraak.tensor.html#var-mapx">mapx</a>.</p></div></div><div class="src-link"><a href="https://github.com/ysmiraak/home/blob/master/src/ysmiraak/tensor.clj#L334">view source</a></div></div><div class="public anchor" id="var-flix"><h3>flix</h3><div class="usage"><code>(flix x)</code><code>(flix perm x)</code><code>(flix x a b &amp; abs)</code></div><div class="doc"><div class="markdown"><p>for transposition. the first case flips the 2 outer-most axes. the second case performs the specified permutation. the third case takes an even number of axes and consecutively flips each two.</p></div></div><div class="src-link"><a href="https://github.com/ysmiraak/home/blob/master/src/ysmiraak/tensor.clj#L155">view source</a></div></div><div class="public anchor" id="var-inat"><h3>inat</h3><div class="usage"><code>(inat d i)</code></div><div class="doc"><div class="markdown"><p>ensures that integer <em>i</em> is positive. used for implementing negative indexing. works under the condition that <code>-d-1 &lt; i &lt; d</code>. see <a href="ysmiraak.tensor.html#var-jnat">jnat</a> for an inclusive variant.</p></div></div><div class="src-link"><a href="https://github.com/ysmiraak/home/blob/master/src/ysmiraak/tensor.clj#L48">view source</a></div></div><div class="public anchor" id="var-jnat"><h3>jnat</h3><div class="usage"><code>(jnat d i)</code></div><div class="doc"><div class="markdown"><p>like <a href="ysmiraak.tensor.html#var-inat">inat</a> but inclusive, namely <code>-d-1 &lt;= i &lt;= d</code>. used for implementing negative slicing.</p></div></div><div class="src-link"><a href="https://github.com/ysmiraak/home/blob/master/src/ysmiraak/tensor.clj#L57">view source</a></div></div><div class="public anchor" id="var-mapx"><h3>mapx</h3><div class="usage"><code>(mapx r f)</code><code>(mapx r f x)</code><code>(mapx r f x &amp; xs)</code></div><div class="doc"><div class="markdown"><p>like <a href="https://clojuredocs.org/clojure.core/mapv">mapv</a> but for tensors, aka higher rank vectors in this implementation. anything thatâ€™s not a <a href="https://clojuredocs.org/clojure.core/vector_q">vector?</a> is treated as a rank-0 tensor, aka a scalar of sorts.</p>
<p>the first argument <em>r</em> is a non-negative integer, which is the rank of the tensors where the function <em>f</em> applies.</p>
<pre><code class="clojure">(mapx 2 inc [[0 1] [2 3]])               =&gt; [[1 2] [3 4]]
(mapx 2  +  [[0 1] [2 3]] [[1 2] [3 4]]) =&gt; [[1 3] [5 7]]
</code></pre>
<p>it does not have to be applied to the lowest level where the scalars live. for a rank-r tensor, any integer between <em>0</em> and <em>r</em> inclusive is applicable.</p>
<pre><code class="clojure">(mapx 1 peek [[0 1] [2 3]]) =&gt; [1 3]
(mapx 0 str  [[0 1] [2 3]]) =&gt; "[[0 1] [2 3]]"
</code></pre>
<p>it may be more intuitive to think of <em>r</em> as the axis instead of the rank. except that a rank-0 tensor has no axis but <code>r = 0</code> is still valid, and, like indices, axes are not inclusive on both ends.</p>
<p>note that the arity-2 case does not produce a transducer, since it operates on vectors and not lazy sequences. instead it produces a partial function. in general <code>(mapx 0 f)</code> is equivalent to <code>f</code> and <code>(partial mapx 1)</code> equivalent to <code>mapv</code>.</p>
<p>see <a href="ysmiraak.tensor.html#var-plux">plux</a> for a similar function which supports negative indexing and broadcasting semantics.</p></div></div><div class="src-link"><a href="https://github.com/ysmiraak/home/blob/master/src/ysmiraak/tensor.clj#L3">view source</a></div></div><div class="public anchor" id="var-plux"><h3>plux</h3><div class="usage"><code>(plux r f x)</code><code>(plux r f x &amp; xs)</code></div><div class="doc"><div class="markdown"><p>like <a href="ysmiraak.tensor.html#var-mapx">mapx</a> but supports broadcasting and negative indexing.</p>
<p>this function can be used to broadcast tensors together.</p>
<pre><code class="clojure">(as-&gt; (plux 0 vector x y z) [x y z] ... )
</code></pre></div></div><div class="src-link"><a href="https://github.com/ysmiraak/home/blob/master/src/ysmiraak/tensor.clj#L233">view source</a></div></div><div class="public anchor" id="var-rank"><h3>rank</h3><div class="usage"><code>(rank x)</code><code>(rank r x)</code></div><div class="doc"><div class="markdown"><p>returns the rank <code>(: Nat)</code> of tensor <em>x</em>, or changes its rank to <em>r</em> by flattening or wrapping the outer-most axes.</p></div></div><div class="src-link"><a href="https://github.com/ysmiraak/home/blob/master/src/ysmiraak/tensor.clj#L65">view source</a></div></div><div class="public anchor" id="var-size"><h3>size</h3><div class="usage"><code>(size x)</code><code>(size ds x)</code></div><div class="doc"><div class="markdown"><p>returns the size <code>(: Nat)</code> of tensor <em>x</em>, or changes its size by broadcasting to shape <em>ds</em>, if the broadcasting logic is satisfied.</p></div></div><div class="src-link"><a href="https://github.com/ysmiraak/home/blob/master/src/ysmiraak/tensor.clj#L136">view source</a></div></div><div class="public anchor" id="var-subx"><h3>subx</h3><div class="usage"><code>(subx x)</code><code>(subx x &amp; subs)</code></div><div class="doc"><div class="markdown"><p>for subscripting. each <em>sub</em> can be one of the following.</p>
<ul>
  <li>
  <p>an integer, for indexing. this collapses the axis itâ€™s applied to thus reduces the tensor rank.</p></li>
  <li>
  <p>a map with <code>:i :j :k</code> keys, for (strided) slicing. the three values are integers, and work analogously to the arguments of <a href="https://clojuredocs.org/clojure.core/range">range</a>, also in the default cases with missing keys. the default values allows an empty map to slice the whole axis.</p></li>
  <li>
  <p>a sequence of booleans, for masking. the sequence needs not be a vector, but must have the same length as the dimension of that axis.</p></li>
  <li>
  <p>a sequence of integers, which returns the entries along that axis in the specified order, allowing duplicates. an empty sequence returns an empty axis. this is the most generic case. any input not of the previous cases is interpreted this way. for example <code>nil</code> is treated as an empty sequence.</p></li>
</ul>
<p>negative indices are supported in all cases where integers are used. but index-out-of-bound errors are not tolerated. the <em>subs</em> are applied consequetively from the outer-most axis to the inner-most.</p></div></div><div class="src-link"><a href="https://github.com/ysmiraak/home/blob/master/src/ysmiraak/tensor.clj#L178">view source</a></div></div><div class="public anchor" id="var-tenx"><h3>tenx</h3><div class="usage"><code>(tenx f t x a &amp; xa*)</code></div><div class="doc"><div class="markdown"><p>tensor manipulation by axis annotation.</p>
<pre><code>f : Tsor s q -&gt; t
t : Vect _ q
x : Tsor s r
a : Vect _ r
 -&gt; Tsor t (r - q)
</code></pre>
<p>each rank-r tensor <em>x</em> is followed by a dim-r vector <em>a</em> which annotates its axes with tags. an annotation tag is an arbitrary value, but must be unique within each annotation vector. across annotation vectors, the same tag identifies the axes to be aligned. those axes must have broadcastable dimensions.</p>
<p><em>t</em> contains tags for extracting axes to which <em>f</em> applies. those tags need not be unique, but only the last occurrence matters. they may also be new tags, in which case dummy axes are created.</p>
<p>if some axes survive extraction, their tags are included in the metadata of the resulting tensor under the <code>:axes</code> key.</p>
<p>assuming the definition of those tensors in the following examples.</p>
<pre><code class="clojure">(def x (dims [4 3 2] (vec (range 24))))
(def y (dims [4 3  ] (vec (range 12))))
(def z (dims [  3 2] (vec (range  6))))
</code></pre>
<ul>
  <li>
  <p>
  </p><p>tensor product</p>
  <pre><code class="clojure">(def xyz (tenx * [] x [:x0 :x1 :x2] y [:y0 :y1] z [:z0 :z1]))
(dims xyz) =&gt; [4 3 2 4 3 3 2]
(meta xyz) =&gt; {:axes [:x0 :x1 :x2 :y0 :y1 :z0 :z1]}
</code></pre><p></p></li>
  <li>
  <p>tensor contraction</p>
  <pre><code class="clojure">(def dot (comp (partial reduce +) (partial map *)))
(dims (tenx dot [3] x [4 3 2] y [4 3] z [3 2])) =&gt; [4 2]
</code></pre></li>
  <li>
  <p>operation along axis</p>
  <pre><code class="clojure">(def sum (partial reduce +))
(dims (tenx sum [1] x [0 1 2])) =&gt; [4 2]
</code></pre></li>
  <li>
  <p>broadcasting</p>
  <pre><code class="clojure">(let [[x' e] (tenx list [4 3 2] x [4 3 2] 0 [])]
  (assert (= (dims x') (dims e)))
  (assert (identical? x' x)))
</code></pre></li>
  <li>
  <p>transposition</p>
  <pre><code class="clojure">(dims (tenx identity [2 1 0] x [0 1 2])) =&gt; [2 3 4]
</code></pre></li>
  <li>
  <p>or this</p>
  <pre><code class="clojure">(tenx identity [0 1 2 3] 0 []) =&gt; [[[[0]]]]
</code></pre></li>
</ul></div></div><div class="src-link"><a href="https://github.com/ysmiraak/home/blob/master/src/ysmiraak/tensor.clj#L250">view source</a></div></div></div></body></html>